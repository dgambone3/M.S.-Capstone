# Detect AI Generated vs Student Written Essays

## Introduction
Generative AI models are rapidly gaining popularity as large-language models (LLMs) advance, becoming more performant and easily accessable. At the forefront of this technology is OpenAI’s free-to-use large-language model, GPT, and its conversational AI chat bot, ChatGPT. 
Though most companies releasing AI powered chatbots do what they can to ensure that their product is safe and responsible in terms of responses and deployment, once the product is in the hands of users the lines of safety and ethical practices become blurred. Many educators have become concerned that chatbots would inflate the presence of cheating, specifically in essay and writing assignments given modern LLM's impressive ability to generate text given a prompt. Educators at Stanford discovered otherwise, citing their recent survey which found cheating has not seen an increase since ChatGPT’s release [4].
Even though these results indicate there hasn’t been a definitive increase in cheating at this point, it is still bene- ficial to invest time and research in identifying whether an essay has been written by a student or generative AI. The same technologies speculated to enable plagiarism could aid in identifying exactly that. This project aims to use both classification machine learning techniques and advanced LLM technologies to determine if an essay has been written by a student or generative AI. This research can also shed light into the capabilities and degree to which AI can generate human-like text.

## Data
The data used for this project is the DAIGT v2 train dataset, hosted on Kaggle [1]. This dataset is a collection of essays in which the AI essays were generated from several different LLM's and the student essays were sourced from the persuade corpus, which contains essays from students in 6th-12th grade on several different essay topics. The AI generated essays were done so using the same essay topic prompts as were given to the students. The class for the dataset is a binary classifier where 0 represents a student written essay, and 1 represents a LLM generated essay. 

## Methods
In terms of preprocessing, basic text cleaning such as removing special characters, numbers, and lowercase was applied to the essays, essays with outlier lengths were removed, and the student class was downsampled to balance the dataset classes. 

The dataset was vectorized using sci-kit learn's TF-IDF vectorizer to take into account word frequence to prepare the features for classification. Naive bayes and logistic regression were used to classify whether an essay was written by a student or generated by AI. Initial baseleines for both models, with little preprocessing or fine tuning, were yeilding very high area under the ROC curve, around 97% or 98%, respectively. 

These unusual and unexpectidly high results yielded further investigation into why the data was so easily seperable. Several possible hypothesis arose that deserved further investigate this:
- Could there be a stark difference in words or level of vocabulary used in AI vs student essays?
- Are the AI vs Student essays more easily seperable within their own class per each prompt?
- What words could be carrying the heaviest weights in the models? Specifically in logistic regression since this model had the highest initial results.
- What is the distribution of words per label? Could some words be only used in AI essays and not student, and vice versa?
- Could typos in student essays be the distinguishing factor between the two classes?

The essays were vectorized with Sci-kit learn's count vectorizer to find the top 25000 features, or more common words. Spell checking was applied to these most frequent words, and stored in a dictionary whenever a word from the top features was found in the spell checker. If spell corrected, the misspelled word and its correction was added to a dictionary to be utilized to replace any occurrances of the misspelled word in the dataset. 

## Results and Exploratory Data Analysis
Initial plots of the distribution for the most frequent words in the dataset appeared to have an even split between words used in student and AI essays, based on word frequency in the dataset.

The naive bayes model resulted in a 94.9% area under the ROC curve on the spell checked text, and 95.5% on the unchecked essays. 
While the logistic regression model resulted in a 98.72% area under the ROC curve on the spell checked text, and 98.7% on the unchecked essays. Learning curve and confusion matricies can be seen below.

Plots of the weights from the logisitic regression model display the top 30 words with the highest absolute value weight. It was expected that possibly these weighted words might shed some light into the data's seperability, by showing the top words to be misspelled or at a very high vocabulary level, but this was not the case. The top words were common words, spelled correctly, such as important, conclusion, or people. 

UMAP was performed to transform high-dimension data into a lower-dimensional space. The UMAP embeddings were used to visualize the prompt data per label, to help visualize the essays per prompt. This showed that, when reduced to lower dimensions, the student essays were more seperarable, with distinguishable clusters. Meanwhile the AI generated essays were not somewhat clustered in lower dimensions, but not as clearly as the student class was. 

Expanding on this plot, visualizing the frequency distribution of these top weighted words does shed some light into the model's results: there were many words with uneven proportions of frequencey used in AI vs student essays. Specifically, many of the top weighted words such as important, conclusion, and potential appeared unproportionally in AI generated essays, while words like reason, cars, and electors appeared more commonly in student written essays. 

Further, the applying spell checker to the essays did slightly affect the results, but not enough to prove that typos were the reason for the dataset's seperability. For example, after the spell check was applied the ROC scores were slightly lower on the spell checked dataset. Spell checking the essays also did slightly affect the words and their order appearing in the most top weighted words, but not by much - some words in a different rank of the top, or exchanging some words appearing in the top weighted words list. 

DistilBERT was trained on the dataset to check the results on inference, this model resulted in an AUC of 99%. Limitations of GPU and comput access did affect the ability to train this model on the complete dataset though. As future research, it would be of interest to dive further into the cause of the high results for the BERT model and to train the model once more resources were available and possibly on a larger dataset. 


### Logistic Regression Result Plots
<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/logreg%20curve%20checked.png" alt="Logistic Regression Curve - Checked" width="400"/>
</kbd>
<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/logreg%20curve%20not%20checked.png" alt="Logistic Regression Curve - Not Checked" width="400"/>
</kbd>

<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/logreg%20matrix%20checked.png" alt="Logistic Regression Confusion Matrix - Checked" width="400"/>
</kbd>
<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/logreg%20matrix%20not%20checked.png" alt="Logistic Regression Confusion Matrix - Not Checked" width="400"/>
</kbd>

### Naive Bayes Result Plots
<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/nb%20curve%20checked.png" alt="Naive Bayes Curve - Checked" width="400"/>
</kbd>

<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/nb%20curve%20not%20checked.png" alt="Naive Bayes Curve - Not Checked" width="400"/>
</kbd>

<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/nb%20matrix%20checked.png" alt="Naive Bayes Confusion Matrix - Checked" width="400"/>
</kbd>

<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/nb%20conf%20matrix%20not%20checked.png" alt="Naive Bayes Confusion Matrix - Not Checked" width="400"/>
</kbd>


### Visualizing Model Weights and Word Counts per Label
<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/top%2050%20not%20spell%20checked%20split.png" alt="Top 50 Words - Not Spell Checked" width="800"/>
</kbd>

<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/top%2050%20spell%20checked%20split.png" alt="Top 50 Words - Spell Checked" width="800"/>
</kbd>
<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/logreg%20weights%20and%20counts%20checked.png" alt="Logistic Regression Weights and Counts - Checked" width="800"/>
</kbd>
<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/logreg%20weights%20and%20counts%20not%20checked.png" alt="Logistic Regression Weights and Counts - Not Checked" width="800"/>
</kbd>

<kbd>
  <img src="https://github.com/dgambone3/M.S.-Capstone/blob/main/images/logreg%20weights%20both.png" alt="Logistic Regression Weights Comparison" width="800"/>
</kbd>


# References
[1] “DAIGT https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset (accessed Jan. 31, 2024).
Kaggle, V2 Train Dataset,”

[2] “LLM - Detect AI Generated Text,” Kaggle, https://www.kaggle.com/competitions/llm-detect-ai-generated-text (accessed Jan. 31, 2024)

[3] “Introducing ChatGPT,” ChatGPT Blog, https://openai.com/blog/chatgpt/ (accessed Feb. 2, 2024).

[4] C. Spector, “What do Ai Chatbots really mean for students and cheating?,” Stanford Graduate School of Education, https://ed.stanford.edu/news/what-do-ai-chatbots-really-mean-students- and-cheating (accessed Feb. 5, 2024).
