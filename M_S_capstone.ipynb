{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgambone3/M.S.-Capstone/blob/main/M_S_capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-gEmmINkpop"
      },
      "source": [
        "# Detecting Student Written vs AI Generated Essays\n",
        "\n",
        "### Can we detect if an essay was written by a student or generated by LLM?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rpjA0iNk1el"
      },
      "source": [
        "# Setup and import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEoTptXWk4GB"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "! pip install pyspellchecker\n",
        "! pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOXIAfSqk7BJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js6fTpYvg8JD"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S-Ails_hFXX"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d thedrcat/daigt-v2-train-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUEmy2sjhIZo"
      },
      "outputs": [],
      "source": [
        "! unzip daigt-v2-train-dataset.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knwDAQ6hVsPK"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuCy0GnOJFKw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "# torch.cuda.device_count()\n",
        "# torch.cuda.current_device()\n",
        "# torch.cuda.device(0)\n",
        "# torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRRprqjAqD3T"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRO5ocnMn-HM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJuxBPQVlB99"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train_v2_drcat_02.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqy5_-T66vq1"
      },
      "outputs": [],
      "source": [
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS2AnqQk7Dwi"
      },
      "outputs": [],
      "source": [
        "label_counts = data['label'].value_counts()\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX1vAcgEvO8n"
      },
      "outputs": [],
      "source": [
        "label_0 = data[data['label'] == 0]\n",
        "label_1 = data[data['label'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jrvhhX1vDXF"
      },
      "outputs": [],
      "source": [
        "def check_unique(label_0_text, label_1_text, type):\n",
        "  # set of words in student essays\n",
        "  student_set_checked = set()\n",
        "  for row in label_0_text:\n",
        "    for word in row.split(\" \"):\n",
        "      student_set_checked.add(word)\n",
        "\n",
        "\n",
        "  # set of words in ai essays\n",
        "  ai_set_checked = set()\n",
        "  for row in label_1_text:\n",
        "    for word in row.split(\" \"):\n",
        "      ai_set_checked.add(word)\n",
        "\n",
        "  # words unique to only student essays (not included in ai)\n",
        "  unique_student_set_checked = student_set_checked - ai_set_checked\n",
        "\n",
        "  # words unique to only ai essays (exclude student words)\n",
        "  unique_ai_set_checked = ai_set_checked - student_set_checked\n",
        "\n",
        "  print(f\"Set unique student words {type}: \",len(unique_student_set_checked))\n",
        "  print(f\"Set unique ai words {type}: \",len(unique_ai_set_checked))\n",
        "\n",
        "\n",
        "\n",
        "check_unique(label_0['text'], label_1['text'], 'not preprocessed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lykHdaxrqSYz"
      },
      "source": [
        "## Basic text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqtFeGB6i5Us"
      },
      "outputs": [],
      "source": [
        "# basic text preprocessing\n",
        "data['text'] = data['text'].str.replace('\\xa0', ' ')\n",
        "data['text'] = data['text'].str.replace('&', 'and')\n",
        "data['text'] = data['text'].str.replace('-', ' ')\n",
        "data['text'] = data['text'].str.replace('\\n', ' ')\n",
        "data['text'] = data['text'].str.replace('\\r', ' ')\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x).lower())\n",
        "# data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnxKqBlove56"
      },
      "outputs": [],
      "source": [
        "label_0 = data[data['label'] == 0]\n",
        "label_1 = data[data['label'] == 1]\n",
        "\n",
        "check_unique(label_0['text'], label_1['text'], 'preprocessed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzgA5vWWI073"
      },
      "source": [
        "**Student essays have more unique words and larger vocabulary (10x)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_temp = pd.read_csv(\"train_v2_drcat_02.csv\")\n",
        "\n",
        "ax = data_temp.label.value_counts().plot(kind='bar')\n",
        "\n",
        "ax.patches[0].set_facecolor('blue')  # Set color for label 0\n",
        "ax.patches[1].set_facecolor('green') # Set color for label 1\n",
        "\n",
        "plt.title(\"Label Distribution - Unbalanced\")\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Counts')\n",
        "ax.set_xticklabels(['Student', 'AI'])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "puCRexpkpdQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gLqyn2gtayF"
      },
      "source": [
        "## boxplots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-AJEycPwkdw"
      },
      "outputs": [],
      "source": [
        "def calculate_length(text):\n",
        "    return len(text.split(\" \"))\n",
        "data['len'] = data['text'].apply(lambda x: calculate_length(x))\n",
        "data.len.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTmKfVTbsxVc"
      },
      "outputs": [],
      "source": [
        "label_0 = data[data['label'] == 0]\n",
        "label_1 = data[data['label'] == 1]\n",
        "data_to_plot_0 = label_0['len']\n",
        "data_to_plot_1 = label_1['len']\n",
        "\n",
        "# boxplot for label student\n",
        "plt.boxplot(data_to_plot_0, positions=[1], widths=0.6, patch_artist=True, boxprops=dict(facecolor='blue'))\n",
        "\n",
        "# boxplot for ai\n",
        "plt.boxplot(data_to_plot_1, positions=[2], widths=0.6, patch_artist=True, boxprops=dict(facecolor='green'))\n",
        "\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Length')\n",
        "plt.title('Boxplots Before Removing Outliers')\n",
        "plt.xticks([1, 2], ['Student', 'AI'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7lcMNc0wiHE"
      },
      "source": [
        "### remove outlier lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5orzLGgiwlqu"
      },
      "outputs": [],
      "source": [
        "data2 = data[data['len'] <= 500]\n",
        "data2 = data2[data2['len'] >= 200]\n",
        "data2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjqoKxnjwtwX"
      },
      "outputs": [],
      "source": [
        "data=data2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpxZw_olSKR4"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oau1ykqaXMwZ"
      },
      "outputs": [],
      "source": [
        "label_0 = data[data['label'] == 0]\n",
        "label_1 = data[data['label'] == 1]\n",
        "data_to_plot_0 = label_0['len']\n",
        "data_to_plot_1 = label_1['len']\n",
        "\n",
        "# boxplot for label student\n",
        "plt.boxplot(data_to_plot_0, positions=[1], widths=0.6, patch_artist=True, boxprops=dict(facecolor='blue'))\n",
        "\n",
        "# boxplot for ai\n",
        "plt.boxplot(data_to_plot_1, positions=[2], widths=0.6, patch_artist=True, boxprops=dict(facecolor='green'))\n",
        "\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Length')\n",
        "plt.title('Boxplots After Removing Outliers')\n",
        "plt.xticks([1, 2], ['Student', 'AI'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmccTZJZBiRe"
      },
      "source": [
        "## label distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-kObP8dfkNm"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "ax = data.label.value_counts().plot(kind='bar')\n",
        "\n",
        "# Customizing bar colors\n",
        "ax.patches[0].set_facecolor('blue')  # Set color for label 0\n",
        "ax.patches[1].set_facecolor('green') # Set color for label 1\n",
        "\n",
        "# Adding labels and title\n",
        "plt.title(\"Label Distribution - Unbalanced\")\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Counts')\n",
        "ax.set_xticklabels(['Student', 'AI'])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1yZxdtOXZP9"
      },
      "source": [
        "### Downsample student essays to balance dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KIPtZxC_zOP"
      },
      "outputs": [],
      "source": [
        "# separate into 2 dfs based on label (cleaned)\n",
        "label_0 = data[data['label'] == 0]\n",
        "label_1 = data[data['label'] == 1]\n",
        "\n",
        "count_label_0 = len(label_0)\n",
        "count_label_1 = len(label_1)\n",
        "\n",
        "if count_label_0 > count_label_1:\n",
        "  label_0_downsampled = label_0.sample(n=count_label_1, random_state=42)\n",
        "  df_balanced = pd.concat([label_0_downsampled, label_1], axis=0)\n",
        "else:\n",
        "  df_balanced = pd.concat([label_0, label_1], axis=0)\n",
        "\n",
        "data = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQMTpE7gqvqs"
      },
      "outputs": [],
      "source": [
        "print('student essays: ', count_label_0)\n",
        "print('ai essays: ', count_label_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3R39c0OfniS"
      },
      "outputs": [],
      "source": [
        "ax = data.label.value_counts().plot(kind='bar')\n",
        "\n",
        "ax.patches[0].set_facecolor('blue')  # Set color for label 0\n",
        "ax.patches[1].set_facecolor('green') # Set color for label 1\n",
        "\n",
        "plt.title(\"Label Distribution - Balanced\")\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Counts')\n",
        "ax.set_xticklabels(['Student', 'AI'])\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIJ732PQ_0N2"
      },
      "outputs": [],
      "source": [
        "# double check balanced\n",
        "label_counts = data['label'].value_counts()\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5XHWwJfqa6b"
      },
      "source": [
        "## Calculate average essay length per label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLh2E8WCC07o"
      },
      "outputs": [],
      "source": [
        "label_0['len'] = label_0['text'].apply(lambda x: calculate_length(x))\n",
        "label_1['len'] = label_1['text'].apply(lambda x: calculate_length(x))\n",
        "\n",
        "print('avg len student essays: ', round(np.mean(label_0.len),2))\n",
        "print('avg len ai essays: ', round(np.mean(label_1.len),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r8V7Ar5I5DB"
      },
      "source": [
        "**Student essays are longer on average**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEK8t9jSnPgf"
      },
      "outputs": [],
      "source": [
        "# group by 'source' and count occurrences of each 'label'\n",
        "grouped_data = data.groupby(['source', 'label']).size().unstack(fill_value=0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 6))\n",
        "grouped_data.plot(kind='bar', ax=ax, color=['blue', 'green'])\n",
        "\n",
        "plt.xlabel('Source')\n",
        "plt.ylabel('Label Counts')\n",
        "plt.title('Label Counts by Source')\n",
        "ax.set_xticklabels(grouped_data.index, rotation=45)\n",
        "\n",
        "# adjust legend labels\n",
        "legend_labels = {0: 'Student', 1: 'AI'}\n",
        "plt.legend(title='Label', labels=[legend_labels[label] for label in grouped_data.columns])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVhHNqGqXC5Z"
      },
      "source": [
        "## UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDuAPgAjQoCO"
      },
      "outputs": [],
      "source": [
        "data['prompt_name'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHlwkLumgyJq"
      },
      "source": [
        "### Student essays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97T_YfzgbN1P"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "# filter only student essays\n",
        "student_data = data[data['label'] == 0]\n",
        "\n",
        "# tfidf matrix\n",
        "student_vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                                  max_features=100,\n",
        "                                  ngram_range=(1, 1))\n",
        "tfidf_features_student = student_vectorizer.fit_transform(student_data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZu_gdh5dsnv"
      },
      "outputs": [],
      "source": [
        "# umap to reduce dimensionality\n",
        "umap_reducer_student = umap.UMAP(random_state=42)\n",
        "umap_embedding_student = umap_reducer_student.fit_transform(tfidf_features_student)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zStELdm8dTeT"
      },
      "outputs": [],
      "source": [
        "# plot umap - prompt\n",
        "for prompt_name in student_data['prompt_name'].unique():\n",
        "    plt.scatter(umap_embedding_student[student_data['prompt_name'] == prompt_name][:, 0],\n",
        "                umap_embedding_student[student_data['prompt_name'] == prompt_name][:, 1],\n",
        "                label=prompt_name, marker='.', linewidths=0.0000000000001)\n",
        "\n",
        "plt.title('UMAP of student essays by prompt name')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend(title='Prompt Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voRu9OKx5kG2"
      },
      "outputs": [],
      "source": [
        "# plot umap - label\n",
        "for label in student_data['label'].unique():\n",
        "    plt.scatter(umap_embedding_student[student_data['label'] == label][:, 0],\n",
        "                umap_embedding_student[student_data['label'] == label][:, 1],\n",
        "                label=label, marker='.', linewidths=0.0000000000001)\n",
        "\n",
        "plt.title('UMAP of student essays by label')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend(title='Prompt Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qvMU_7Dg0Bj"
      },
      "source": [
        "### ai essays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAtfVr3fgjx9"
      },
      "outputs": [],
      "source": [
        "# filter for only ai essays\n",
        "ai_data = data[data['label'] == 1]\n",
        "\n",
        "# tf-idf\n",
        "ai_vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                                      max_features=100,\n",
        "                                      ngram_range=(1, 1))\n",
        "tfidf_features_ai = ai_vectorizer.fit_transform(ai_data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3QHuvg-gjo9"
      },
      "outputs": [],
      "source": [
        "# umap to reduce dimensionality\n",
        "umap_reducer_ai = umap.UMAP(random_state=42)\n",
        "umap_embedding_ai = umap_reducer_ai.fit_transform(tfidf_features_ai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_V5RlqgjTp"
      },
      "outputs": [],
      "source": [
        "# plot umap - prompt\n",
        "for prompt_name in ai_data['prompt_name'].unique():\n",
        "    plt.scatter(umap_embedding_ai[ai_data['prompt_name'] == prompt_name][:, 0],\n",
        "                umap_embedding_ai[ai_data['prompt_name'] == prompt_name][:, 1],\n",
        "                label=prompt_name, marker='.', linewidths=0.0000000000001)\n",
        "\n",
        "plt.title('UMAP of ai essays by prompt name')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend(title='Prompt Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvsidPEg5qRs"
      },
      "outputs": [],
      "source": [
        "# plot umap - label\n",
        "for label in ai_data['label'].unique():\n",
        "    plt.scatter(umap_embedding_ai[ai_data['label'] == label][:, 0],\n",
        "                umap_embedding_ai[ai_data['label'] == label][:, 1],\n",
        "                label=label, marker='.', linewidths=0.0000000000001)\n",
        "\n",
        "plt.title('UMAP of ai essays by label')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend(title='Prompt Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXx406hlz3C"
      },
      "source": [
        "### whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8_ojVdjhelo"
      },
      "outputs": [],
      "source": [
        "# tfidf\n",
        "vectorizer_all = TfidfVectorizer(stop_words='english',\n",
        "                                  max_features=100,\n",
        "                                  ngram_range=(1, 1))\n",
        "tfidf_features_all = vectorizer_all.fit_transform(data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqfzgxcVhecX"
      },
      "outputs": [],
      "source": [
        "# umap to reduce dimensionality\n",
        "umap_reducer_all = umap.UMAP(random_state=42)\n",
        "umap_embedding_all = umap_reducer_all.fit_transform(tfidf_features_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXCFWYaA6TB-"
      },
      "outputs": [],
      "source": [
        "# plot umap - prompt\n",
        "for prompt_name in data['prompt_name'].unique():\n",
        "    plt.scatter(umap_embedding_all[data['prompt_name'] == prompt_name][:, 0],\n",
        "                umap_embedding_all[data['prompt_name'] == prompt_name][:, 1],\n",
        "                label=prompt_name, marker='.', linewidths=0.0000000000001)\n",
        "\n",
        "plt.title('UMAP Visualization all essays by prompt')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVRUcemtheQo"
      },
      "outputs": [],
      "source": [
        "# plot umap - label\n",
        "for label in data['label'].unique():\n",
        "    plt.scatter(umap_embedding_all[data['label'] == label][:, 0],\n",
        "                umap_embedding_all[data['label'] == label][:, 1],\n",
        "                label=label, marker='.', linewidths=0.000000000000000000001)\n",
        "\n",
        "plt.title('UMAP Visualization all essays by Label')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lATZHyOgpL7"
      },
      "source": [
        "## Apply spell checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KDOEX_Tbg0-"
      },
      "outputs": [],
      "source": [
        "get_vocab = CountVectorizer(stop_words='english',\n",
        "                            lowercase=True,\n",
        "                            strip_accents=\"ascii\"\n",
        "                            )\n",
        "\n",
        "get_vocab.fit_transform(data['text'])\n",
        "vocab_dict = get_vocab.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_dict)"
      ],
      "metadata": {
        "id": "BINy20jN-Yz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBVmqxBdgOi4"
      },
      "outputs": [],
      "source": [
        "vocab = list(vocab_dict.keys())\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rkq9fcoivMJ"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "spell = SpellChecker()\n",
        "# correction_counts = {}\n",
        "\n",
        "def spell_check(vocabulary):\n",
        "  # corrected_words = {}\n",
        "  with tqdm(total=len(vocab), desc=\"Spell Checking\") as pbar:\n",
        "    for word in vocabulary:\n",
        "      if word in typos:\n",
        "        continue\n",
        "      checked = spell.correction(word)\n",
        "      if checked is not None and checked != word and \"'\" not in checked and checked not in list(typos.keys()):\n",
        "        typos[\" \" + word + \" \"] = \" \" + checked + \" \"\n",
        "      pbar.update(1)\n",
        "  return typos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLWCbRtbbjCa"
      },
      "outputs": [],
      "source": [
        "# add words to be recognized by spell checker based on initial spell check results\n",
        "spell.word_frequency.load_words(['microsoft', 'apple', 'google', 'theres', 'california', 'nasa', 'theyre','ohio',\n",
        "                                 'todays', 'thats', 'im', 'american','theyre', 'venus', 'texting','europe','ive',\n",
        "                                 'wasnt','clinton','thomas','wyoming','donald', 'youre', 'paris', 'america', 'bogota',\n",
        "                                 'americans', 'mona', 'lisa','richard','hillary', 'luke', 'apps', 'obama', 'whos', 'florida',\n",
        "                                 'romney','nasas', 'texas', 'george', 'americas', 'partys', 'itll', 'carlos', 'nixon',\n",
        "                                 'andrew','andrews', 'lifes', 'extracurriculars'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyDrtRH0U0gv"
      },
      "outputs": [],
      "source": [
        "# create dictionary with {typo:correct} for all spell checked words in top 25000 features\n",
        "typos = spell_check(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdMAbSF4RymQ"
      },
      "outputs": [],
      "source": [
        "def apply_spell_check(text, typos):\n",
        "  text_corr = \"\"\n",
        "  for word in typos.keys():\n",
        "    if word in text:\n",
        "      corrected_word = typos[word]\n",
        "      text = text.replace(word, corrected_word)\n",
        "      if corrected_word not in typo_count:\n",
        "        typo_count[corrected_word] = 1\n",
        "      else:\n",
        "        typo_count[corrected_word] += 1\n",
        "  return text\n",
        "\n",
        "# temp = 'testing becuse is spell checked'\n",
        "# print(apply_spell_check(temp, typos))\n",
        "# print(typo_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahMJIxZ1d7pn"
      },
      "outputs": [],
      "source": [
        "# replace typos identified in top 25,000 words with their corrected value from dict\n",
        "typo_count = {}\n",
        "data['text_spell_checked'] = data['text'].progress_apply(lambda x : apply_spell_check(x, typos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avOCq8SaeFCE"
      },
      "outputs": [],
      "source": [
        "# create dictionary of top words corrected\n",
        "top_typos = {k: abs(v) for k, v in sorted(typo_count.items(), key=lambda item: abs(item[1]), reverse=True)}\n",
        "top_typos = dict(list(top_typos.items()))\n",
        "len(top_typos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhEDUSxDequ3"
      },
      "outputs": [],
      "source": [
        "def get_key_by_value(dictionary, search_value):\n",
        "    for key, value in dictionary.items():\n",
        "        if value == search_value:\n",
        "            return key\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNmYX2aYezTh"
      },
      "outputs": [],
      "source": [
        "top_corrected_dict = {}\n",
        "for key in top_typos.keys():\n",
        "  top_corrected_dict[key] = get_key_by_value(typos, key)\n",
        "\n",
        "# top_corrected_dict # correct : typo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del top_typos[' do ']\n",
        "del top_typos[' their ']\n",
        "del top_typos[' were ']\n",
        "del top_typos[' else ']\n",
        "del top_typos[' this ']\n",
        "del top_typos[' the ']\n",
        "\n",
        "del top_corrected_dict[' the ']\n",
        "del top_corrected_dict[' do ']\n",
        "del top_corrected_dict[' their ']\n",
        "del top_corrected_dict[' were ']\n",
        "del top_corrected_dict[' else ']\n",
        "del top_corrected_dict[' this ']"
      ],
      "metadata": {
        "id": "dAy36PEQ1c1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOYwn7AloLRU"
      },
      "outputs": [],
      "source": [
        "typo_df = pd.DataFrame({'corrected word': top_corrected_dict.keys(),\n",
        "                        'misspelled': top_corrected_dict.values(),\n",
        "                        'times corrected': top_typos.values()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2gKAWY6nIWE"
      },
      "outputs": [],
      "source": [
        "# combined = [str(key.strip()) + \" : \" + str(value.strip()) for key, value in top_corrected_dict.items()]\n",
        "# combined = dict(zip(combined, top_typos.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGf6tC6ynwse"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(20, 6))\n",
        "# plt.bar(list(combined.keys())[:30], list(combined.values())[:30])\n",
        "# plt.xlabel('Typos and their Corrections')\n",
        "# plt.ylabel('times corrected')\n",
        "# plt.xticks(rotation=75)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM4nX0N9vJxl"
      },
      "outputs": [],
      "source": [
        "# check unique words after spell checking\n",
        "label_0 = data[data['label'] == 0]\n",
        "label_1 = data[data['label'] == 1]\n",
        "\n",
        "check_unique(label_0['text'], label_1['text'], 'not spell checked')\n",
        "print('........................................')\n",
        "check_unique(label_0['text_spell_checked'], label_1['text_spell_checked'], 'spell checked')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnbS7lfjsZRQ"
      },
      "source": [
        "## Visualize top (most common) words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA-DBJkB8k05"
      },
      "outputs": [],
      "source": [
        "essays_text = data['text']\n",
        "essays_check = data['text_spell_checked']\n",
        "labels = data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCDZ3LcIkO_C"
      },
      "outputs": [],
      "source": [
        "def visualize_top_words(essays, classes, type):\n",
        "  count = CountVectorizer(stop_words='english',\n",
        "                          lowercase=True,\n",
        "                          strip_accents=\"ascii\",\n",
        "                          max_features=3000\n",
        "                          )\n",
        "  X = essays\n",
        "  y = classes\n",
        "  X = count.fit_transform(essays)\n",
        "\n",
        "  feature_names = count.get_feature_names_out()\n",
        "  word_counts = dict(zip(feature_names, X.sum(axis=0).A1))\n",
        "  sorted_vocab = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "  # extract the top 50 items\n",
        "  top_50 = dict(list(sorted_vocab.items())[:50])\n",
        "\n",
        "  plt.figure(figsize=(20, 6))\n",
        "  plt.bar(top_50.keys(), top_50.values())\n",
        "  plt.xlabel('Words')\n",
        "  plt.ylabel('Count')\n",
        "  plt.title(f'Top 50 Words by Count - {type}')\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.yticks(range(0, 80001, 10000))\n",
        "  plt.show()\n",
        "\n",
        "  return X, y, feature_names, top_50\n",
        "\n",
        "\n",
        "X_main, y_main, feature_names_count, top_50_words = visualize_top_words(essays_text, labels, 'not spell checked')\n",
        "X_check, y_check, feature_names_count_check, top_50_words_check = visualize_top_words(essays_check, labels, 'spell checked')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(list(top_50_words.items()), columns=['word', 'count'])"
      ],
      "metadata": {
        "id": "xQ5QmEQowmyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(list(top_50_words_check.items()), columns=['word', 'count'])"
      ],
      "metadata": {
        "id": "9R4j4En4wrB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Ldrdf3n2Vp"
      },
      "source": [
        "## Create dataframe to hold unique words / vocabulary to visualize word distributions by label (student or AI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGCiagi9l2Ac"
      },
      "outputs": [],
      "source": [
        "def create_word_df(X, y, feature_names):\n",
        "  # combine data for both labels\n",
        "  word_scores_combined = []\n",
        "  for label in [0, 1]:\n",
        "      label_indices = (y == label)\n",
        "      label_X = X[label_indices]\n",
        "      label_word_scores = label_X.sum(axis=0)\n",
        "      word_scores_combined.append(label_word_scores)\n",
        "\n",
        "  # convert combined label scores to array\n",
        "  word_scores_combined = np.array(word_scores_combined).squeeze()\n",
        "\n",
        "  # creating a dataframe with words and their scores\n",
        "  word_df = pd.DataFrame({'word': feature_names,\n",
        "                          'score_label_0': word_scores_combined[0],\n",
        "                          'score_label_1': word_scores_combined[1]})\n",
        "  totals = []\n",
        "  for i in range(word_df.shape[0]):\n",
        "    totals.append(word_df.score_label_0[i] + word_df.score_label_1[i])\n",
        "\n",
        "  word_df['totals'] = totals\n",
        "  word_df=word_df.sort_values(by='totals', ascending=False)\n",
        "  return word_df\n",
        "\n",
        "\n",
        "\n",
        "word_df_main = create_word_df(X_main, y_main, feature_names_count)\n",
        "word_df_check = create_word_df(X_check, y_check, feature_names_count_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpuY-c3xsuTS"
      },
      "source": [
        "### Plot top words split by label, spell checked and not spell checked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC2RJvC3nfi8"
      },
      "outputs": [],
      "source": [
        "def plot_words_split_label(word_df, type):\n",
        "  short_word_df = word_df[:50]\n",
        "\n",
        "  df = short_word_df.sort_values(by='totals', ascending=False)\n",
        "\n",
        "  plt.figure(figsize=(20, 6))\n",
        "  # plot bars for score_label_0\n",
        "  plt.bar(df['word'], df['score_label_0'], label='student', color='blue')\n",
        "  # plot bars for score_label_1 on top of the previous ones\n",
        "  plt.bar(df['word'], df['score_label_1'], bottom=df['score_label_0'], label='ai', color='green')\n",
        "\n",
        "  plt.xlabel('Word')\n",
        "  plt.ylabel('Counts')\n",
        "  plt.title(f'Top {len(short_word_df)} Most Common Words for Both Labels - {type}')\n",
        "  plt.legend()\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.show()\n",
        "\n",
        "plot_words_split_label(word_df_main, 'not spell checked')\n",
        "plot_words_split_label(word_df_check, 'not spell checked')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0To-7CAyHLO"
      },
      "source": [
        "# ML Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ley6UwyN4S"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import mean_squared_error as MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3srqnNVwB74"
      },
      "source": [
        "## Output result methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E75kXsTHi_ni"
      },
      "outputs": [],
      "source": [
        "def get_learning_curve(model, X, y):\n",
        "  scores=[]\n",
        "  perc=[]\n",
        "  for n in range(1,101):\n",
        "    perc.append(n)\n",
        "    XX = X[0:int(X.shape[0] * (n/100))] #df\n",
        "    yy = y[0:int(len(y) * (n/100))] #list\n",
        "    pred = model.predict(XX)\n",
        "    score = MSE(y_true=yy, y_pred=pred)\n",
        "    scores.append(score)\n",
        "  return pd.DataFrame({'percent':perc, 'scores':scores})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFcCOzFPjAd1"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curves(model, name, X_train, y_train, X_test, y_test):\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  pred = model.predict(X_test)\n",
        "\n",
        "  # calculate test error on test set\n",
        "  test_error = MSE(y_test, pred)\n",
        "\n",
        "  # get training and testing scores by calling learning curve function\n",
        "  train_scores = get_learning_curve(model, X_train, y_train)\n",
        "  test_scores = get_learning_curve(model, X_test, y_test)\n",
        "\n",
        "  # plot train\n",
        "  plt.plot(train_scores['percent'],\n",
        "          train_scores['scores'],\n",
        "          label = \"train\")\n",
        "\n",
        "  # plot test\n",
        "  plt.plot(test_scores['percent'],\n",
        "          test_scores['scores'],\n",
        "          label = \"test\")\n",
        "\n",
        "  plt.xlabel('Sample Size (%)')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(f'Learning Curve for {name}')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Z-ZipmJa0i"
      },
      "outputs": [],
      "source": [
        "class_labels = {0 : 'student', 1 : \"ai\"}\n",
        "def plot_confusion_matrix(model, y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[class_labels[i] for i in model.classes_])\n",
        "  disp.plot()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_hRUcK4R-V9"
      },
      "source": [
        "## Train models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMtOe1uD0C7u"
      },
      "outputs": [],
      "source": [
        "# data[['text', 'text_spell_checked','label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_7DUG_pR6-J"
      },
      "outputs": [],
      "source": [
        "essays_text = data['text']\n",
        "essays_check = data['text_spell_checked']\n",
        "labels = data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KUjLySYoedj"
      },
      "outputs": [],
      "source": [
        "def models(essays, label, type):\n",
        "  tfidf = TfidfVectorizer(stop_words='english',\n",
        "                        lowercase=True,\n",
        "                        # max_features=5000,\n",
        "                        strip_accents=\"ascii\"\n",
        "                        )\n",
        "  X = essays\n",
        "  y = label\n",
        "  X = tfidf.fit_transform(essays)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  nb = BernoulliNB()\n",
        "  nb_pred = plot_learning_curves(nb, f\"Naive Bayes - {type}\", X_train, y_train, X_test, y_test)\n",
        "  plot_confusion_matrix(nb, y_test, nb_pred)\n",
        "\n",
        "  print(classification_report(y_test, nb_pred))\n",
        "  print(f'Accuracy: {np.round(accuracy_score(nb_pred, y_test) * 100, 2)}%')\n",
        "  print(f'Area under ROC curve: {np.round(roc_auc_score(nb_pred, y_test) * 100, 2)}%')\n",
        "\n",
        "\n",
        "  logreg = LogisticRegression(solver='sag')\n",
        "  logreg_pred = plot_learning_curves(logreg, f\"Logistic Regression - {type}\", X_train, y_train, X_test, y_test)\n",
        "  plot_confusion_matrix(logreg, y_test, logreg_pred)\n",
        "  print(f'Accuracy: {np.round(accuracy_score(logreg_pred, y_test) * 100, 2)}%')\n",
        "  print(f'Area under ROC curve: {np.round(roc_auc_score(logreg_pred, y_test) * 100, 2)}%')\n",
        "  print(classification_report(y_test, logreg_pred))\n",
        "\n",
        "  coefficients = logreg.coef_[0]\n",
        "\n",
        "  # get feature names (words)\n",
        "  feature_names = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "  return coefficients, feature_names\n",
        "\n",
        "\n",
        "coeff, feature_names_tfidf = models(essays_text, labels, 'not spell checked')\n",
        "coeff_check, feature_names_tfidf_check = models(essays_check, labels, 'spell checked')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzUa2N64uku4"
      },
      "source": [
        "### Visualize model weights for Logistic Regression to analyze what features may be impacting the model most or have the highest weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoan0CpQqw3Y"
      },
      "outputs": [],
      "source": [
        "def viz_weights(feat_name, coeffs, type):\n",
        "  # zip feature names and coefficients to analyze\n",
        "  feature_coefficient = dict(zip(feat_name, coeffs))\n",
        "\n",
        "  # sort df by weights to get top word features\n",
        "  sorted_feature_coefficients = {k: v for k, v in sorted(feature_coefficient.items(), key=lambda item: abs(item[1]), reverse=True)}\n",
        "  weighted_words = list(sorted_feature_coefficients.keys())[:30]\n",
        "\n",
        "  coefficients = list(sorted_feature_coefficients.values())[:30] # top 30 weighted words\n",
        "  abs_weights = [abs(ele) for ele in coefficients] # already abs\n",
        "\n",
        "  plt.figure(figsize=(15,4))\n",
        "  plt.plot(weighted_words, abs_weights)\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.xlabel('Word')\n",
        "  plt.ylabel('Weight (absolute value)')\n",
        "  plt.title(f'Logistic Regression Weights - {type}')\n",
        "  plt.show()\n",
        "  return weighted_words, abs_weights\n",
        "\n",
        "word_wts_main, abs_wts_main = viz_weights(feature_names_tfidf, coeff, 'not spell checked')\n",
        "word_wts_check, abs_wts_check = viz_weights(feature_names_tfidf_check, coeff_check, 'spell checked')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX4yghrVhn6s"
      },
      "outputs": [],
      "source": [
        "def viz_weights(feat_name, coeffs, type):\n",
        "    feature_coefficient = dict(zip(feat_name, coeffs))\n",
        "    sorted_feature_coefficients = {k: v for k, v in sorted(feature_coefficient.items(), key=lambda item: abs(item[1]), reverse=True)}\n",
        "    weighted_words = list(sorted_feature_coefficients.keys())[:30]\n",
        "    coefficients = list(sorted_feature_coefficients.values())[:30]\n",
        "    abs_weights = [abs(ele) for ele in coefficients]\n",
        "\n",
        "    return weighted_words, abs_weights\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
        "\n",
        "word_wts_main, abs_wts_main = viz_weights(feature_names_tfidf, coeff, 'not spell checked')\n",
        "word_wts_check, abs_wts_check = viz_weights(feature_names_tfidf_check, coeff_check, 'spell checked')\n",
        "\n",
        "ax1.plot(word_wts_main, abs_wts_main, color='blue', label='not spell checked')\n",
        "ax1.set_xticklabels(word_wts_main, rotation=45)\n",
        "ax1.set_ylabel('Weight (absolute value)')\n",
        "ax1.set_title('Logistic Regression Weights')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(word_wts_check, abs_wts_check, color='green', label='spell checked')\n",
        "ax2.set_xticklabels(word_wts_check, rotation=45)\n",
        "ax2.set_xlabel('Word')\n",
        "ax2.set_ylabel('Weight (absolute value)')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkWE4Zk4Fruw"
      },
      "outputs": [],
      "source": [
        "word_weight_main = dict(zip(word_wts_main, abs_wts_main))\n",
        "word_weight_check = dict(zip(word_wts_check, abs_wts_check))\n",
        "\n",
        "weight_df_main = pd.DataFrame.from_dict(word_weight_main, orient='index')\n",
        "weight_df_main = weight_df_main.rename_axis('word').reset_index().rename(columns={0: \"weight_main\"})\n",
        "\n",
        "weight_df_check = pd.DataFrame.from_dict(word_weight_check, orient='index')\n",
        "weight_df_check = weight_df_check.rename_axis('word').reset_index().rename(columns={0: \"weight_check\"})\n",
        "\n",
        "weight_df = pd.merge(weight_df_main, weight_df_check, on='word',how='outer')\n",
        "\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Weight (absolute value)')\n",
        "plt.title(f'Logistic Regression Weights Overlay Same Order as not spell checked')\n",
        "plt.plot(weight_df['word'], weight_df['weight_main'], label='not spell checked', color='blue')\n",
        "plt.plot(weight_df['word'], weight_df['weight_check'], label='spell checked', color='green')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# weight_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16aA651kfGQV"
      },
      "source": [
        "## Create dataframe to store weights to view words grouped by label (student or AI)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "typo_"
      ],
      "metadata": {
        "id": "wZoVwtuF6CFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhgKzL2LGNpA"
      },
      "outputs": [],
      "source": [
        "def viz_merged_label(weight_df, word_df, col, type):\n",
        "  merged_df = pd.merge(weight_df, word_df, on='word', how='left')\n",
        "  fig, ax1 = plt.subplots(figsize=(20, 6))\n",
        "\n",
        "  # plot bars for score_label_0 and score_label_1\n",
        "  ax1.bar(merged_df['word'], merged_df['score_label_0'], label='student', color='blue')\n",
        "  ax1.bar(merged_df['word'], merged_df['score_label_1'], bottom=merged_df['score_label_0'], label='ai', color='green')\n",
        "  plt.xticks(rotation=45)\n",
        "  # twin axis\n",
        "  ax2 = ax1.twinx()\n",
        "\n",
        "  # plot the line on the second axis\n",
        "  ax2.plot(weight_df['word'], weight_df[col], color='red', label=type)\n",
        "\n",
        "  ax1.set_xlabel('Word')\n",
        "  ax1.set_ylabel('Counts')\n",
        "  ax2.set_ylabel('Weights')\n",
        "  ax1.set_title(f'Distribution of Top Weighted Words based on Label and logreg model - {type}')\n",
        "\n",
        "  # combine legends from both axes\n",
        "  lines, labels = ax1.get_legend_handles_labels()\n",
        "  lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "  ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "weight_df['weight_main'] = weight_df['weight_main'].fillna(0.0)\n",
        "weight_df['weight_check'] = weight_df['weight_check'].fillna(0.0)\n",
        "\n",
        "viz_merged_label(weight_df[weight_df['weight_main'] > 0], word_df_main, 'weight_main', 'not spell checked')\n",
        "viz_merged_label(weight_df[weight_df['weight_check'] > 0], word_df_check, 'weight_check', 'spell checked')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWfz2tNakXFC"
      },
      "outputs": [],
      "source": [
        "label_0 = data[data['label'] == 0]\n",
        "label_1 = data[data['label'] == 1]\n",
        "\n",
        "\n",
        "check_unique(label_0['text'], label_1['text'], 'not spell checked')\n",
        "print('........................................')\n",
        "check_unique(label_0['text_spell_checked'], label_1['text_spell_checked'], 'spell checked')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3LDURfzCuGV"
      },
      "outputs": [],
      "source": [
        "temp_typo_df=typo_df[['corrected word', 'times corrected']]\n",
        "temp_typo_df.rename(columns={'corrected word':'word'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3E-VrTwmgU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7meomI8Ketl"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "_-3Wd0MPn1ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-eiHhM4t1kg"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "import torch\n",
        "from transformers import DataCollatorWithPadding\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SSRwB50ErA"
      },
      "source": [
        "downsampling dataset for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2ET2q7kzxVO"
      },
      "outputs": [],
      "source": [
        "# percent = 5\n",
        "# data = data.sample(frac=(percent/100))\n",
        "# print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaoTpb4rq4xs"
      },
      "outputs": [],
      "source": [
        "data_simple = data[['text','label']].copy()\n",
        "# data_simple.head()\n",
        "\n",
        "train_df, test_df = train_test_split(data_simple, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SYLwNi3OIGf"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "max_length = tokenizer.model_max_length  # Get the maximum sequence length supported by the tokenizer\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding=True,  # Enable dynamic padding\n",
        "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "        max_length=512 # otherwise shape too large for distilbert model\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbZdocnuwFI4"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mli_1HdItVmX"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHotwakdropf"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFCZqTWlr75R"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"student\", 1: \"ai\"}\n",
        "label2id = {\"student\": 0, \"ai\": 1}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEXzXpSQxx4r"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Log training loss and evaluation metrics\n",
        "        if state.is_world_process_zero and logs is not None:\n",
        "            print(f\"Training Loss: {logs.get('loss')}, Eval Loss: {logs.get('eval_loss')}, Eval Accuracy: {logs.get('eval_accuracy')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scS15gBcr-j0"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"capstone_model_small\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2, #16\n",
        "    per_device_eval_batch_size=2, #16\n",
        "    num_train_epochs=10, # 2\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[TrainerCallback()],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoeL0nndoZHz"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# !pip install numba\n",
        "\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "XjBTs7EBfZZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmm5ooyGvAd1"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6LbV66WUG10"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDqCwPiIT_CV"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-skjcnuIzg_T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from transformers import AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItNWTUDxxDTz"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = test_dataset[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIXHIozCxKG8"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "domJnBUnxLMh"
      },
      "outputs": [],
      "source": [
        "predicted_probabilities = predictions.predictions[:, 1]  # Assuming the positive class is index 1\n",
        "auc = roc_auc_score(true_labels, predicted_probabilities)\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byjJVJF53XXZ"
      },
      "outputs": [],
      "source": [
        "train_logs = trainer.state.log_history\n",
        "# print(type(train_logs))\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "df = pd.DataFrame(train_logs)\n",
        "# print(df.head())\n",
        "\n",
        "loss_df = df[df['loss'].notna()]\n",
        "# print(len(loss_df))\n",
        "\n",
        "# Plot the loss\n",
        "plt.plot(loss_df['epoch'], loss_df['loss'], label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJO-AT0_Xus6"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "car_free_cities_prompt = \"\"\"Write an explanatory essay to inform fellow citizens\n",
        "                          about the advantages of limiting car usage. Your essay\n",
        "                          must be based on ideas and information that can be found\n",
        "                          in the passage set. Manage your time carefully so that\n",
        "                          you can read the passages; plan your response; write your\n",
        "                          response; and revise and edit your response. Be sure to use\n",
        "                          evidence from multiple sources; and avoid overly relying on\n",
        "                          one source. Your response should be in the form of a multiparagraph\n",
        "                          essay. Write your essay in the space provided.\"\"\"\n",
        "\n",
        "gpt_essay = \"\"\"It's super important to think about using cars less because it can help our\n",
        "planet and make things better for everyone. When we use cars less, it's like giving a big\n",
        "high-five to the environment, our wallets, and even our communities! First off, using cars\n",
        "less can make the air cleaner. Cars let out stuff called pollutants that can make the air\n",
        "dirty and not good to breathe. When we don't use cars as much, there's less of that yucky\n",
        "stuff in the air, which is awesome for our health and the planet. Also, using cars less\n",
        "can help with traffic. Have you ever been stuck in a big line of cars that doesn't move? That's\n",
        "called traffic, and it's no fun! When we don't drive as much, there are fewer cars on the road,\n",
        "which means less traffic jams. That means we can get where we need to go faster and without all\n",
        "the honking and frustration. Another cool thing about using cars less is that it can save us\n",
        "money. Cars cost a lot of money to buy, put gas in, and fix when they break. But if we don't\n",
        "use them as much, we don't have to spend as much money on them. That means we can save money\n",
        "for other fun things, like going on trips or buying snacks! And guess what? Using cars less\n",
        "can even help make our communities better. When we walk or bike instead of driving everywhere,\n",
        "we get to see our neighborhood up close. We might even run into friends or neighbors along the\n",
        "way! Plus, walking and biking are good exercise, so it's good for our bodies too. So, it's pretty\n",
        "clear that using cars less is a super smart idea. It helps keep the air clean, makes traffic less\n",
        "of a headache, saves us money, and brings our communities closer together. Let's all try to use\n",
        "cars a little less and make our world a better place!\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"dgambone/capstone_model\")\n",
        "\n",
        "classifier(gpt_essay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq8670PQ9XHq"
      },
      "outputs": [],
      "source": [
        "# ######### TEMP ##########\n",
        "# # import torch\n",
        "\n",
        "# batch_size = 16\n",
        "# num_samples = len(test_dataset[\"text\"])\n",
        "# predicted_labels = []\n",
        "\n",
        "# for i in range(0, num_samples, batch_size):\n",
        "#     batch_texts = test_dataset[\"text\"][i:i+batch_size]\n",
        "#     inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#     batch_predictions = np.argmax(outputs.logits.cpu().numpy(), axis=1)\n",
        "#     predicted_labels.extend(batch_predictions)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPXCPJ0pjtpPm5v4SxPixB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}